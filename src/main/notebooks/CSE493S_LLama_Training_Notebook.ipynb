{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLaMa Training & Generation Code\n",
        "### Alex Mous and Elliott Zackrone\n",
        "All code replicated from repo [LLaMa-Train](https://github.com/alex-mous/LLaMa-Train/) with changes to work with new structure and loading from Google Drive. "
      ],
      "metadata": {
        "id": "uk-cbNOwEcg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "ywgXKHJJEu4A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ebOkiq0I3F",
        "outputId": "bb677b72-ed1e-400a-8aa6-cdf5456d9b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.22.4)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.0.1+cu118)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->xformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->xformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: sentencepiece, mypy-extensions, typing-inspect, pyre-extensions, xformers\n",
            "Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.29 sentencepiece-0.1.99 typing-inspect-0.9.0 xformers-0.0.20\n"
          ]
        }
      ],
      "source": [
        "!pip install xformers sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model (Llama folder)"
      ],
      "metadata": {
        "id": "DPAoTr5AEywK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xformers_model.py"
      ],
      "metadata": {
        "id": "W4sTHKcuE0PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Reduced scale single GPU LLaMa model with XFormers efficient attention and rotary embedding\n",
        "Based off of original LLaMa model\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import xformers.ops as xops\n",
        "from xformers.components.positional_embedding import RotaryEmbedding\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 512\n",
        "    n_layers: int = 8\n",
        "    n_heads: int = 8\n",
        "    vocab_size: int = -1  # defined later by tokenizer\n",
        "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
        "    norm_eps: float = 1e-5\n",
        "\n",
        "    max_batch_size: int = 32\n",
        "    max_seq_len: int = 2048\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super(RMSNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "\n",
        "def xformers_attn(xq, xk, xv, is_causal):\n",
        "    mask = xops.LowerTriangularMask() if is_causal else None\n",
        "    return xops.memory_efficient_attention(\n",
        "        xq, xk, xv, attn_bias=mask\n",
        "    )\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.n_local_heads = args.n_heads\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(\n",
        "            args.dim,\n",
        "            3 * args.n_heads * self.head_dim,\n",
        "            bias=False\n",
        "        )\n",
        "        self.out_proj = nn.Linear(\n",
        "            args.n_heads * self.head_dim,\n",
        "            args.dim,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.pos_embed = RotaryEmbedding(self.head_dim)\n",
        "        self.attn_fn = xformers_attn\n",
        "\n",
        "    def forward(self, x: torch.Tensor, is_causal: bool = False):\n",
        "        bsz, seqlen, _ = x.shape\n",
        "        xq, xk, xv = self.in_proj(x).chunk(3, dim=-1)\n",
        "\n",
        "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "\n",
        "        xq, xk = self.pos_embed(xq, xk)\n",
        "        output = self.attn_fn(\n",
        "            xq.to(xv.dtype),\n",
        "            xk.to(xv.dtype),\n",
        "            xv,\n",
        "            is_causal=is_causal\n",
        "        )\n",
        "\n",
        "        output = output.view(bsz, seqlen, -1)\n",
        "\n",
        "        return self.out_proj(output)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        hidden_dim: int,\n",
        "        multiple_of: int,\n",
        "    ):\n",
        "        super(FeedForward, self).__init__()\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "\n",
        "        self.w1 = nn.Linear(\n",
        "            in_features=dim,\n",
        "            out_features=hidden_dim,\n",
        "            bias=False\n",
        "        )\n",
        "        self.w2 = nn.Linear(\n",
        "            in_features=hidden_dim,\n",
        "            out_features=dim,\n",
        "            bias=False\n",
        "        )\n",
        "        self.w3 = nn.Linear(\n",
        "            in_features=dim,\n",
        "            out_features=hidden_dim,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, layer_id: int, args: ModelArgs):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.attention = Attention(args)\n",
        "        self.feed_forward = FeedForward(\n",
        "            dim=args.dim,\n",
        "            hidden_dim=4 * args.dim,\n",
        "            multiple_of=args.multiple_of\n",
        "        )\n",
        "        self.layer_id = layer_id\n",
        "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, is_causal: bool = True):\n",
        "        x_res = x + self.attention(self.attention_norm(x), is_causal)\n",
        "        out = x_res + self.feed_forward(self.ffn_norm(x_res))\n",
        "        return out\n",
        "\n",
        "\n",
        "class XFormersTransformer(nn.Module):\n",
        "    def __init__(self, params: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        self.vocab_size = params.vocab_size\n",
        "        self.n_layers = params.n_layers\n",
        "\n",
        "        self.token_embeddings = nn.Embedding(\n",
        "            num_embeddings=params.vocab_size,\n",
        "            embedding_dim=params.dim\n",
        "        )\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for layer_id in range(params.n_layers):\n",
        "            self.layers.append(TransformerBlock(layer_id, params))\n",
        "\n",
        "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
        "        self.output = nn.Linear(\n",
        "            in_features=params.dim,\n",
        "            out_features=params.vocab_size,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, is_causal: bool = True):\n",
        "        x = self.token_embeddings(tokens)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, is_causal)\n",
        "        x = self.norm(x)\n",
        "        output = self.output(x)  # compute logits for all instead of just last\n",
        "        return output.float()\n"
      ],
      "metadata": {
        "id": "Ra2cXxz6HAhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer.py"
      ],
      "metadata": {
        "id": "NdK99IozE-8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "SentencePieceProcessor-based Tokenizer, based off of original LLaMa tokenizer\n",
        "\"\"\"\n",
        "\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "from typing import List\n",
        "import os\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, model_path: str):\n",
        "        # Load tokenizer from tokenizer model\n",
        "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
        "\n",
        "        # Copy special tokens from model\n",
        "        self.n_words: int = self.sp_model.vocab_size()\n",
        "        self.bos_id: int = self.sp_model.bos_id()\n",
        "        self.eos_id: int = self.sp_model.eos_id()\n",
        "        self.pad_id: int = self.sp_model.pad_id()\n",
        "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
        "\n",
        "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
        "        assert type(s) is str\n",
        "        t = self.sp_model.encode(s)\n",
        "        if bos:\n",
        "            t = [self.bos_id] + t\n",
        "        if eos:\n",
        "            t = t + [self.eos_id]\n",
        "        return t\n",
        "\n",
        "    def decode(self, t: List[int]) -> str:\n",
        "        return self.sp_model.decode(t)"
      ],
      "metadata": {
        "id": "XehZxTU7RB0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama.py"
      ],
      "metadata": {
        "id": "hlh4GcXIFAgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from typing import Tuple, Optional\n",
        "# from src.main.llama import Transformer, XFormersTransformer, Tokenizer, ModelArgs\n",
        "\n",
        "\n",
        "def load_llama(\n",
        "        tokenizer_path: str,\n",
        "        initial_checkpoint: Optional[str],\n",
        "        use_xformers: bool = False,\n",
        "        **model_args\n",
        ") -> Tuple[nn.Module, Tokenizer]:\n",
        "    # Load LLaMa model and tokenizer with given parameters\n",
        "    start_time = time.time()\n",
        "    print(\"Loading LLaMa model and tokenizer\")\n",
        "    tokenizer = Tokenizer(model_path=tokenizer_path)\n",
        "    model_params = ModelArgs(**model_args)\n",
        "    model_params.vocab_size = tokenizer.n_words\n",
        "    if use_xformers:\n",
        "        model = XFormersTransformer(model_params)\n",
        "    else:\n",
        "        model = None\n",
        "    torch.set_default_tensor_type(torch.FloatTensor)\n",
        "    if initial_checkpoint is not None:\n",
        "        torch.load(initial_checkpoint, map_location=\"cpu\")\n",
        "    print(f\"Loaded model and tokenizer in {time.time() - start_time:.2f} seconds\")\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "MMNdFFtfDbqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing (Util folder)"
      ],
      "metadata": {
        "id": "ZFTZaX17RHR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data.py"
      ],
      "metadata": {
        "id": "Two-UnwhETPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data processing and loading\n",
        "\"\"\"\n",
        "\n",
        "from typing import Tuple, Optional\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "default_data_path: str = \".\"\n",
        "\n",
        "artifacts_path: str = \"artifacts/\"\n",
        "\n",
        "\n",
        "class PileDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for loading the Pile dataset.\n",
        "    Loaded from an array of sequences, each of equal length.\n",
        "    \"\"\"\n",
        "    def __init__(self, seqs: torch.Tensor):\n",
        "        self.seqs = seqs\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= self.__len__():\n",
        "            return None\n",
        "        return self.seqs[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.seqs.shape[0]\n",
        "\n",
        "\n",
        "def _tokenize_line(line: str, tokenizer: Tokenizer, max_seq_len: int, pad_id: int):\n",
        "    # Tokenize a string line into at least one sequence of max_seq_len and return tensor of sequences\n",
        "    line_tokens = torch.tensor(tokenizer.encode(line, bos=True, eos=False)).long()\n",
        "    if len(line_tokens) > max_seq_len:  # split into multiple sequences\n",
        "        line_tokens = line_tokens[:max_seq_len * (len(line_tokens) // max_seq_len)]  # trim to multiple\n",
        "        line_tokens = line_tokens.view(max_seq_len, -1).t()  # reshape into (num_seq, max_seq_len)\n",
        "    else:\n",
        "        line_tokens = line_tokens.reshape(1, -1)  # reshape into (1, seq len)\n",
        "    tokens = torch.full((line_tokens.shape[0], max_seq_len), pad_id).long()\n",
        "    for i, t in enumerate(line_tokens):\n",
        "        tokens[i, : min(max_seq_len, len(t))] = t\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def process_file(\n",
        "        tokenizer: Tokenizer,\n",
        "        data_file: str,\n",
        "        max_seqs: int = 20000,\n",
        "        max_seq_len: int = 2048\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Process JSONL file into up to max_seqs seqs of tokens of length seq_len\n",
        "    Returns tensor of sequences, each of seq_len with padding of tokenizer eos id\n",
        "    :param tokenizer:\n",
        "    :param data_file:\n",
        "    :param max_seqs:\n",
        "    :param max_seq_len:\n",
        "    :return: Tensor of dimension (up to max_seqs, seq_len)\n",
        "    \"\"\"\n",
        "\n",
        "    # check if corresponding artifact exists.\n",
        "    artifact_path = os.path.join(artifacts_path, f\"{os.path.splitext(data_file)[0]}.pt\")\n",
        "    if os.path.isfile(artifact_path):\n",
        "        print(f\"Artifact tokens found. Loading tokenized dataset from {artifact_path}\")\n",
        "        return torch.load(artifact_path)\n",
        "    # otherwise, parse file.\n",
        "    print(f\"No artifact found. Loading and tokenizing dataset from {data_file}.\")\n",
        "\n",
        "    # create artifacts dir if they don't exist\n",
        "    if not os.path.isdir(artifacts_path):\n",
        "        os.makedirs(artifacts_path)\n",
        "\n",
        "    seqs = torch.zeros((1, max_seq_len), dtype=torch.long)  # sequences to parse\n",
        "    pad_id = tokenizer.eos_id  # padding id\n",
        "\n",
        "    # process data file into tokenized sequences padded to exactly max_seq_len\n",
        "    with open(data_file, \"r\", encoding=\"utf-8\") as file:\n",
        "        with tqdm(total=max_seqs, desc=\"Dataset loading: \") as p_bar:\n",
        "            for jsonline in file:\n",
        "                if seqs.shape[0] >= max_seqs:\n",
        "                    break\n",
        "                raw = json.loads(jsonline)\n",
        "                tokens = _tokenize_line(raw[\"text\"], tokenizer, max_seq_len, pad_id)\n",
        "                seqs = torch.vstack((seqs, tokens))\n",
        "                p_bar.update(tokens.shape[0])\n",
        "\n",
        "    # save artifact and return\n",
        "    torch.save(seqs[1:], artifact_path)\n",
        "    return seqs[1:]\n",
        "\n",
        "\n",
        "def load_pile_dataset(\n",
        "        tokenizer: Tokenizer,\n",
        "        train_file : str,\n",
        "        val_file : str,\n",
        "        test_file : str = \"\",\n",
        "        num_train: int = 20000,\n",
        "        num_val: int = 10000,\n",
        "        num_test: int = 0,\n",
        "        max_seq_len: int = 2048,\n",
        "        data_path: str = default_data_path\n",
        ") -> Tuple[PileDataset, PileDataset, Optional[PileDataset]]:\n",
        "    \"\"\"\n",
        "    Load Pile dataset into train, val, and test datasets of tokens\n",
        "    with numbers of sequences and sequence lengths as specified\n",
        "    :param max_seq_len:\n",
        "    :param data_path:\n",
        "    :param test_file:\n",
        "    :param val_file:\n",
        "    :param train_file:\n",
        "    :param tokenizer:\n",
        "    :param num_train:\n",
        "    :param num_val:\n",
        "    :param num_test:\n",
        "    :param max_seq_len:\n",
        "    :return: train, val, (optionally) test PileDatasets\n",
        "    \"\"\"\n",
        "    print(\"Loading Pile dataset...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_toks = process_file(tokenizer, os.path.join(data_path, train_file), num_train, max_seq_len)\n",
        "    val_toks = process_file(tokenizer, os.path.join(data_path, val_file), num_val, max_seq_len)\n",
        "    test = None\n",
        "    if num_test > 0:\n",
        "        test_toks = process_file(tokenizer, os.path.join(data_path, test_file), num_test, max_seq_len)\n",
        "        test = PileDataset(test_toks)\n",
        "    train = PileDataset(train_toks)\n",
        "    val = PileDataset(val_toks)\n",
        "\n",
        "    print(f\"Loaded dataset in {time.time() - start_time:.2f} seconds\")\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "def get_pile_dataloaders(train_set: PileDataset, val_set: PileDataset, test_set: PileDataset = None, batch_size: int = 32):\n",
        "    \"\"\"\n",
        "    Get dataloaders for train, val, and test datasets with given batch size\n",
        "    :param train_set:\n",
        "    :param val_set:\n",
        "    :param test_set:\n",
        "    :param batch_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "KOA25pNrRIog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics.py"
      ],
      "metadata": {
        "id": "cn_7VB_BEXZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute metrics based on a model and dataset loader\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#from src.main.llama import Transformer\n",
        "\n",
        "\n",
        "def accuracy(model: nn.Module, dataset_loader: DataLoader, device: torch.device) -> float:\n",
        "    \"\"\"\n",
        "    Calculate accuracy of model based on dataset loader\n",
        "    :param model:\n",
        "    :param dataset_loader:\n",
        "    :param device:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def compute_loss(model: XFormersTransformer, tokens: torch.Tensor, loss_fn : nn.CrossEntropyLoss) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute loss on the input batch of tokens\n",
        "    :param model:\n",
        "    :param tokens:\n",
        "    :param loss_fn:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    logits = model.forward(tokens[:, :-1], is_causal=True)\n",
        "    flattened_logits = logits.reshape(-1, model.params.vocab_size)  # flatten logits for input to cross-entropy loss\n",
        "    shift_tokens = tokens[:, 1:].reshape(-1)  # shift tokens so we only compute loss after first token\n",
        "    loss = loss_fn(flattened_logits, shift_tokens)  # compute loss between logits and true tokens, ignoring padding\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_number_of_parameters(model: nn.Module):\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if parameter.requires_grad:\n",
        "            total_params += parameter.numel()\n",
        "    return total_params\n"
      ],
      "metadata": {
        "id": "Z-SeLiliDUOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints.py"
      ],
      "metadata": {
        "id": "T4e5RRaQERSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load and save checkpoints for model and optimizer\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "def save_checkpoint(optimizer: optim.Optimizer, model: nn.Module, checkpoint_path: str):\n",
        "    \"\"\"\n",
        "    Save checkpoint from optimizer and model into checkpoint_path\n",
        "\n",
        "    :param optimizer: Optimizer used during training\n",
        "    :param model: PyTorch model\n",
        "    :param checkpoint_path: Checkpoint path and name\n",
        "    \"\"\"\n",
        "    if checkpoint_path is not None:\n",
        "        torch.save({\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model': model.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(optimizer: optim.Optimizer, model: nn.Module, checkpoint_path: str):\n",
        "    \"\"\"\n",
        "    Load checkpoint into optimizer and model from checkpoint_path\n",
        "\n",
        "    :param optimizer: Optimizer used during training (optional)\n",
        "    :param model: PyTorch model\n",
        "    :param checkpoint_path: Checkpoint path and name\n",
        "    \"\"\"\n",
        "    if checkpoint_path is not None:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "\n",
        "def generate_checkpoint_name(checkpoints_base_path: str, epoch: int):\n",
        "    \"\"\"\n",
        "    Generate a checkpoint name for the given model type and epoch\n",
        "\n",
        "    :param checkpoints_base_path: Path to directory to store checkpoints in\n",
        "    :param model: PyTorch model\n",
        "    :return: Checkpoint path and name\n",
        "    \"\"\"\n",
        "    return os.path.join(checkpoints_base_path, f\"chkpt-{epoch}.pt\")\n"
      ],
      "metadata": {
        "id": "iAPTxaXCMz8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Batches"
      ],
      "metadata": {
        "id": "X5u67M0QENfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL CODE - NOT REQUIRED FOR TRAINING\n",
        "# Show example batches from Pile dataset\n",
        "tokenizer_path = \"tokenizer.model\"\n",
        "train_path = \"tiny_train.jsonl\"\n",
        "val_path = \"tiny_val.jsonl\"\n",
        "assert os.path.isfile(tokenizer_path), \"LLaMa tokenizer pretrained model file required\"\n",
        "assert os.path.isfile(train_path), \"Train data subset in JSONL format required\"\n",
        "assert os.path.isfile(val_path), \"Validation data subset in JSONL format required\"\n",
        "batch_size = 4\n",
        "max_seq_len = 512\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model, tokenizer = load_llama(\n",
        "    tokenizer_path=tokenizer_path,\n",
        "    initial_checkpoint=None,\n",
        "    use_xformers=True,\n",
        "    max_seq_len=max_seq_len\n",
        ")\n",
        "    \n",
        "train_set, val_set, _ = load_pile_dataset(\n",
        "    tokenizer,\n",
        "    train_path,\n",
        "    val_path,\n",
        "    num_train=20000,\n",
        "    num_val=10000,\n",
        "    max_seq_len=max_seq_len,\n",
        ")\n",
        "train_dataloader, val_dataloader, _ = get_pile_dataloaders(\n",
        "    train_set,\n",
        "    val_set,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(\"Example data point\")\n",
        "print(train_set[0])\n",
        "print(\"Example train batch\")\n",
        "print(next(iter(train_dataloader)))\n",
        "print(\"Example val batch\")\n",
        "print(next(iter(val_dataloader)))"
      ],
      "metadata": {
        "id": "AbwKBPH8XLXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "NR1hYUIZRLpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "# Ensure that folder LLaMaTrain exists in My Drive, with tokenizer.model, tiny_train.jsonl, and tiny_val.jsonl before running the following training code\n",
        "from google.colab import drive\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "persistent_storage_path = \"/gdrive/MyDrive/LLaMaTrain/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whRBJTgnQaZ6",
        "outputId": "c544c949-2583-4c90-bb1b-4210607707ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "NpHolcdGD94T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Optional\n",
        "import torch\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# from src.main.llama import XFormersTransformer, Tokenizer, load_llama\n",
        "# from src.main.util import get_pile_dataloaders, load_pile_dataset\n",
        "# from src.main.util import compute_loss\n",
        "# from src.main.util import save_checkpoint, load_checkpoint, generate_checkpoint_name\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def train(\n",
        "        model: XFormersTransformer,\n",
        "        tokenizer: Tokenizer,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        epochs: int,\n",
        "        lr: float,\n",
        "        weight_decay: float,\n",
        "        grad_clip: float = 1.0,\n",
        "        checkpoints_dir: str = None\n",
        "):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    loss_fn = CrossEntropyLoss(ignore_index=tokenizer.eos_id)  # ignore padding\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    #lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs*len(train_loader))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        try:\n",
        "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "                # compute loss between predictions (logits) and tokens\n",
        "                # each prediction corresponds to the next token, so we shift tokens by one\n",
        "                tokens = batch.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                loss = compute_loss(model, tokens, loss_fn)  # compute logits on all using all but last token\n",
        "                # tokens.cpu()  # ensure tokens can be garbage collected\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                optimizer.step()\n",
        "                #lr_scheduler.step()  # update lr\n",
        "                train_loss += loss.item()\n",
        "        finally:\n",
        "            # garbage collect to process next batch\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            try:\n",
        "                for batch in val_loader:\n",
        "                    tokens = batch.to(device)\n",
        "                    val_loss += compute_loss(model, tokens, loss_fn).item()\n",
        "                    tokens.cpu()\n",
        "            finally:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Print summary\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}. Train loss: {train_loss}. Val loss: {val_loss}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        chkpt_path = generate_checkpoint_name(checkpoints_dir, epoch+1)\n",
        "        torch.save(model, chkpt_path)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Model, data, and tokenizer arguments\n",
        "    tokenizer_path = \"tokenizer.model\"\n",
        "    train_path = \"tiny_train.jsonl\"\n",
        "    val_path = \"tiny_val.jsonl\"\n",
        "    checkpoint_base_path = persistent_storage_path + \"checkpoints/\"\n",
        "    checkpoint_run_name = \"dim-512-heads-8-layers-8\"  # relative to base path\n",
        "    load_checkpoint_path = None #\"run2/chkpt-3.pt\"  # relative to base path\n",
        "    assert os.path.isfile(persistent_storage_path + tokenizer_path), \"LLaMa tokenizer pretrained model file required\"\n",
        "    assert os.path.isfile(persistent_storage_path + train_path), \"Train data subset in JSONL format required\"\n",
        "    assert os.path.isfile(persistent_storage_path + val_path), \"Validation data subset in JSONL format required\"\n",
        "    epochs = 15\n",
        "    batch_size = 16\n",
        "    lr = 3e-4\n",
        "    weight_decay = 0.01\n",
        "    max_seq_len = 512\n",
        "    dim = 512\n",
        "    n_layers = 8\n",
        "    n_heads = 8\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    initial_checkpoint = os.path.join(checkpoint_base_path, load_checkpoint_path) if load_checkpoint_path is not None else None\n",
        "    model, tokenizer = load_llama(\n",
        "        tokenizer_path=persistent_storage_path + tokenizer_path,\n",
        "        initial_checkpoint=initial_checkpoint,\n",
        "        use_xformers=True,\n",
        "        max_seq_len=max_seq_len,\n",
        "        dim=dim,\n",
        "        n_layers=n_layers,\n",
        "        n_heads=n_heads\n",
        "    )\n",
        "\n",
        "    train_set, val_set, _ = load_pile_dataset(\n",
        "        tokenizer,\n",
        "        persistent_storage_path + train_path,\n",
        "        persistent_storage_path + val_path,\n",
        "        num_train=20000,\n",
        "        num_val=10000,\n",
        "        max_seq_len=max_seq_len,\n",
        "    )\n",
        "    train_dataloader, val_dataloader, _ = get_pile_dataloaders(\n",
        "        train_set,\n",
        "        val_set,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    checkpoints_dir = os.path.join(checkpoint_base_path, checkpoint_run_name)\n",
        "    if not os.path.isdir(checkpoints_dir):\n",
        "        os.makedirs(checkpoints_dir)\n",
        "\n",
        "    try:\n",
        "        train(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            train_dataloader,\n",
        "            val_dataloader,\n",
        "            lr=lr,\n",
        "            epochs=epochs,\n",
        "            weight_decay=weight_decay,\n",
        "            checkpoints_dir=checkpoints_dir\n",
        "        )\n",
        "    finally:\n",
        "        # Ensure model is on CPU so it can be garbage collected\n",
        "        model.cpu()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    finally:\n",
        "        # Cleanup before exiting\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "xnxb1CRxRMby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "3wRb2g0_D_1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "6adysCBYEBcu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}