{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLaMa Training & Generation Code\n",
        "### Alex Mous and Elliott Zackrone\n",
        "All code replicated from repo [LLaMa-Train](https://github.com/alex-mous/LLaMa-Train/) with changes to work with new structure and loading from Google Drive. "
      ],
      "metadata": {
        "id": "uk-cbNOwEcg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "ywgXKHJJEu4A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2ebOkiq0I3F"
      },
      "outputs": [],
      "source": [
        "!pip install xformers sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model (Llama folder)"
      ],
      "metadata": {
        "id": "DPAoTr5AEywK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xformers_model.py"
      ],
      "metadata": {
        "id": "W4sTHKcuE0PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Reduced scale single GPU LLaMa model with XFormers efficient attention and rotary embedding\n",
        "Based off of original LLaMa model\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import xformers.ops as xops\n",
        "from xformers.components.positional_embedding import RotaryEmbedding\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 512\n",
        "    n_layers: int = 8\n",
        "    n_heads: int = 8\n",
        "    vocab_size: int = -1  # defined later by tokenizer\n",
        "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
        "    norm_eps: float = 1e-5\n",
        "\n",
        "    max_batch_size: int = 32\n",
        "    max_seq_len: int = 2048\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super(RMSNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "\n",
        "def xformers_attn(xq, xk, xv, is_causal):\n",
        "    mask = xops.LowerTriangularMask() if is_causal else None\n",
        "    return xops.memory_efficient_attention(\n",
        "        xq, xk, xv, attn_bias=mask\n",
        "    )\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.n_local_heads = args.n_heads\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(\n",
        "            args.dim,\n",
        "            3 * args.n_heads * self.head_dim,\n",
        "            bias=False\n",
        "        )\n",
        "        self.out_proj = nn.Linear(\n",
        "            args.n_heads * self.head_dim,\n",
        "            args.dim,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.pos_embed = RotaryEmbedding(self.head_dim)\n",
        "        self.attn_fn = xformers_attn\n",
        "\n",
        "    def forward(self, x: torch.Tensor, is_causal: bool = False):\n",
        "        bsz, seqlen, _ = x.shape\n",
        "        xq, xk, xv = self.in_proj(x).chunk(3, dim=-1)\n",
        "\n",
        "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
        "\n",
        "        xq, xk = self.pos_embed(xq, xk)\n",
        "        output = self.attn_fn(\n",
        "            xq.to(xv.dtype),\n",
        "            xk.to(xv.dtype),\n",
        "            xv,\n",
        "            is_causal=is_causal\n",
        "        )\n",
        "\n",
        "        output = output.view(bsz, seqlen, -1)\n",
        "\n",
        "        return self.out_proj(output)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        hidden_dim: int,\n",
        "        multiple_of: int,\n",
        "    ):\n",
        "        super(FeedForward, self).__init__()\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "\n",
        "        self.w1 = nn.Linear(\n",
        "            in_features=dim,\n",
        "            out_features=hidden_dim,\n",
        "            bias=False\n",
        "        )\n",
        "        self.w2 = nn.Linear(\n",
        "            in_features=hidden_dim,\n",
        "            out_features=dim,\n",
        "            bias=False\n",
        "        )\n",
        "        self.w3 = nn.Linear(\n",
        "            in_features=dim,\n",
        "            out_features=hidden_dim,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, layer_id: int, args: ModelArgs):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.attention = Attention(args)\n",
        "        self.feed_forward = FeedForward(\n",
        "            dim=args.dim,\n",
        "            hidden_dim=4 * args.dim,\n",
        "            multiple_of=args.multiple_of\n",
        "        )\n",
        "        self.layer_id = layer_id\n",
        "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, is_causal: bool = True):\n",
        "        x_res = x + self.attention(self.attention_norm(x), is_causal)\n",
        "        out = x_res + self.feed_forward(self.ffn_norm(x_res))\n",
        "        return out\n",
        "\n",
        "\n",
        "class XFormersTransformer(nn.Module):\n",
        "    def __init__(self, params: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        self.vocab_size = params.vocab_size\n",
        "        self.n_layers = params.n_layers\n",
        "\n",
        "        self.token_embeddings = nn.Embedding(\n",
        "            num_embeddings=params.vocab_size,\n",
        "            embedding_dim=params.dim\n",
        "        )\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for layer_id in range(params.n_layers):\n",
        "            self.layers.append(TransformerBlock(layer_id, params))\n",
        "\n",
        "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
        "        self.output = nn.Linear(\n",
        "            in_features=params.dim,\n",
        "            out_features=params.vocab_size,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, is_causal: bool = True):\n",
        "        x = self.token_embeddings(tokens)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, is_causal)\n",
        "        x = self.norm(x)\n",
        "        output = self.output(x)  # compute logits for all instead of just last\n",
        "        return output.float()\n"
      ],
      "metadata": {
        "id": "Ra2cXxz6HAhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer.py"
      ],
      "metadata": {
        "id": "NdK99IozE-8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "SentencePieceProcessor-based Tokenizer, based off of original LLaMa tokenizer\n",
        "\"\"\"\n",
        "\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "from typing import List\n",
        "import os\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, model_path: str):\n",
        "        # Load tokenizer from tokenizer model\n",
        "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
        "\n",
        "        # Copy special tokens from model\n",
        "        self.n_words: int = self.sp_model.vocab_size()\n",
        "        self.bos_id: int = self.sp_model.bos_id()\n",
        "        self.eos_id: int = self.sp_model.eos_id()\n",
        "        self.pad_id: int = self.sp_model.pad_id()\n",
        "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
        "\n",
        "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
        "        assert type(s) is str\n",
        "        t = self.sp_model.encode(s)\n",
        "        if bos:\n",
        "            t = [self.bos_id] + t\n",
        "        if eos:\n",
        "            t = t + [self.eos_id]\n",
        "        return t\n",
        "\n",
        "    def decode(self, t: List[int]) -> str:\n",
        "        return self.sp_model.decode(t)"
      ],
      "metadata": {
        "id": "XehZxTU7RB0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama.py"
      ],
      "metadata": {
        "id": "hlh4GcXIFAgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from typing import Tuple, Optional\n",
        "# from src.main.llama import Transformer, XFormersTransformer, Tokenizer, ModelArgs\n",
        "\n",
        "\n",
        "def load_llama(\n",
        "        tokenizer_path: str,\n",
        "        initial_chkpt: str = None,\n",
        "        use_xformers: bool = False,\n",
        "        new_chkpt: bool = False,  # New checkpointing method\n",
        "        **model_args\n",
        ") -> Tuple[nn.Module, Tokenizer]:\n",
        "    # Load LLaMa model and tokenizer with given parameters\n",
        "    assert use_xformers, \"Only XFormers model supported\"\n",
        "    start_time = time.time()\n",
        "    print(\"Loading LLaMa model and tokenizer\")\n",
        "    tokenizer = Tokenizer(model_path=tokenizer_path)\n",
        "    if initial_chkpt is not None and not new_chkpt:\n",
        "        print(f\"Loading initial checkpoint from {initial_chkpt}\")\n",
        "        model = torch.load(initial_chkpt, map_location=\"cpu\")\n",
        "    else:\n",
        "        model_params = ModelArgs(**model_args)\n",
        "        model_params.vocab_size = tokenizer.n_words\n",
        "        model = XFormersTransformer(model_params)\n",
        "        if initial_chkpt is not None:\n",
        "            model.load_state_dict(torch.load(initial_chkpt))\n",
        "    torch.set_default_tensor_type(torch.FloatTensor)\n",
        "    print(f\"Loaded model and tokenizer in {time.time() - start_time:.2f} seconds\")\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "MMNdFFtfDbqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing (Util folder)"
      ],
      "metadata": {
        "id": "ZFTZaX17RHR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data.py"
      ],
      "metadata": {
        "id": "Two-UnwhETPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data processing and loading\n",
        "\"\"\"\n",
        "\n",
        "from typing import Tuple, Optional\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "default_data_path: str = \".\"\n",
        "\n",
        "artifacts_path: str = \"artifacts/\"\n",
        "\n",
        "\n",
        "class PileDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for loading the Pile dataset.\n",
        "    Loaded from an array of sequences, each of equal length.\n",
        "    \"\"\"\n",
        "    def __init__(self, seqs: torch.Tensor):\n",
        "        self.seqs = seqs\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= self.__len__():\n",
        "            return None\n",
        "        return self.seqs[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.seqs.shape[0]\n",
        "\n",
        "\n",
        "def _tokenize_line(line: str, tokenizer: Tokenizer, max_seq_len: int, pad_id: int):\n",
        "    # Tokenize a string line into at least one sequence of max_seq_len and return tensor of sequences\n",
        "    line_tokens = torch.tensor(tokenizer.encode(line, bos=True, eos=False)).long()\n",
        "    tokens = torch.full((math.ceil(len(line_tokens)/max_seq_len)*max_seq_len, ), pad_id, dtype=torch.long)\n",
        "    tokens[:len(line_tokens)] = line_tokens\n",
        "    tokens = tokens.view(max_seq_len, -1).t()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def process_file(\n",
        "        tokenizer: Tokenizer,\n",
        "        data_file: str,\n",
        "        max_seqs: int = 20000,\n",
        "        max_seq_len: int = 2048\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Process JSONL file into up to max_seqs seqs of tokens of length seq_len\n",
        "    Returns tensor of sequences, each of seq_len with padding of tokenizer eos id\n",
        "    :param tokenizer:\n",
        "    :param data_file:\n",
        "    :param max_seqs:\n",
        "    :param max_seq_len:\n",
        "    :return: Tensor of dimension (up to max_seqs, seq_len)\n",
        "    \"\"\"\n",
        "\n",
        "    # check if corresponding artifact exists.\n",
        "    artifact_path = os.path.join(os.path.normpath(artifacts_path), f\"{os.path.splitext(os.path.basename(data_file))[0]}.pt\")\n",
        "    if os.path.isfile(artifact_path):\n",
        "        print(f\"Artifact tokens found. Loading tokenized dataset from {artifact_path}\")\n",
        "        return torch.load(artifact_path)[:max_seqs]\n",
        "    # otherwise, parse file.\n",
        "    print(f\"No artifact found at {artifact_path}. Loading and tokenizing dataset from {data_file}.\")\n",
        "\n",
        "    # create artifacts dir if they don't exist\n",
        "    if not os.path.isdir(artifacts_path):\n",
        "        os.makedirs(artifacts_path)\n",
        "\n",
        "    pad_id = tokenizer.eos_id  # padding id\n",
        "    seqs = torch.empty((max_seqs, max_seq_len), dtype=torch.long)  # sequences to parse\n",
        "\n",
        "    # process data file into tokenized sequences padded to exactly max_seq_len\n",
        "    curr = 0  # current sequence\n",
        "    with open(data_file, \"r\", encoding=\"utf-8\") as file:\n",
        "        with tqdm(total=max_seqs, desc=\"Dataset loading: \") as p_bar:\n",
        "            for jsonline in file:\n",
        "                if curr >= max_seqs:\n",
        "                    break\n",
        "                raw = json.loads(jsonline)\n",
        "                tokens = _tokenize_line(raw[\"text\"], tokenizer, max_seq_len, pad_id)\n",
        "                num_toks = min(tokens.shape[0], max_seqs-curr)\n",
        "                seqs[curr:curr+num_toks, :] = tokens[:num_toks, :]\n",
        "                curr += num_toks\n",
        "                p_bar.update(num_toks)\n",
        "\n",
        "    # save artifact and return\n",
        "    torch.save(seqs, artifact_path)\n",
        "    return seqs\n",
        "\n",
        "\n",
        "def load_pile_dataset(\n",
        "        tokenizer: Tokenizer,\n",
        "        train_file : str,\n",
        "        val_file : str,\n",
        "        test_file : str = \"\",\n",
        "        num_train: int = 20000,\n",
        "        num_val: int = 10000,\n",
        "        num_test: int = 0,\n",
        "        max_seq_len: int = 2048,\n",
        "        data_path: str = default_data_path\n",
        ") -> Tuple[PileDataset, PileDataset, Optional[PileDataset]]:\n",
        "    \"\"\"\n",
        "    Load Pile dataset into train, val, and test datasets of tokens\n",
        "    with numbers of sequences and sequence lengths as specified\n",
        "    \"\"\"\n",
        "    print(\"Loading Pile dataset...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_toks = process_file(tokenizer, os.path.join(data_path, train_file), num_train, max_seq_len)\n",
        "    val_toks = process_file(tokenizer, os.path.join(data_path, val_file), num_val, max_seq_len)\n",
        "    test = None\n",
        "    if num_test > 0:\n",
        "        test_toks = process_file(tokenizer, os.path.join(data_path, test_file), num_test, max_seq_len)\n",
        "        test = PileDataset(test_toks)\n",
        "    train = PileDataset(train_toks)\n",
        "    val = PileDataset(val_toks)\n",
        "\n",
        "    print(f\"Loaded dataset in {time.time() - start_time:.2f} seconds\")\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "def get_pile_dataloaders(train_set: PileDataset, val_set: PileDataset, test_set: PileDataset = None, batch_size: int = 32):\n",
        "    \"\"\"\n",
        "    Get dataloaders for train, val, and test datasets with given batch size\n",
        "    :param train_set:\n",
        "    :param val_set:\n",
        "    :param test_set:\n",
        "    :param batch_size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "KOA25pNrRIog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics.py"
      ],
      "metadata": {
        "id": "cn_7VB_BEXZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute metrics based on a model and dataset loader\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#from src.main.llama import Transformer\n",
        "\n",
        "\n",
        "def evaluate(model: XFormersTransformer, eval_dataloader: DataLoader, loss_fn: nn.CrossEntropyLoss) -> float:\n",
        "    \"\"\"\n",
        "    Compute loss on eval dataloader\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        try:\n",
        "            for batch in eval_dataloader:\n",
        "                tokens = batch.to(device)\n",
        "                val_loss += compute_loss(model, tokens, loss_fn).item()\n",
        "                tokens.cpu()\n",
        "        finally:\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "    return val_loss / len(eval_dataloader)\n",
        "\n",
        "\n",
        "def compute_loss(model: XFormersTransformer, tokens: torch.Tensor, loss_fn: nn.CrossEntropyLoss) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute loss on the input batch of tokens\n",
        "    :param model:\n",
        "    :param tokens:\n",
        "    :param loss_fn:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    logits = model.forward(tokens[:, :-1], is_causal=True)\n",
        "    flattened_logits = logits.reshape(-1, model.params.vocab_size)  # flatten logits for input to cross-entropy loss\n",
        "    shift_tokens = tokens[:, 1:].reshape(-1)  # shift tokens so we only compute loss after first token\n",
        "    loss = loss_fn(flattened_logits, shift_tokens)  # compute loss between logits and true tokens, ignoring padding\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_number_of_parameters(model: nn.Module):\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if parameter.requires_grad:\n",
        "            total_params += parameter.numel()\n",
        "    return total_params\n"
      ],
      "metadata": {
        "id": "Z-SeLiliDUOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints.py"
      ],
      "metadata": {
        "id": "T4e5RRaQERSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load and save checkpoints for model and optimizer\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "def generate_checkpoint_name(checkpoints_base_path: str, epoch: int, new_type: bool):\n",
        "    \"\"\"\n",
        "    Generate a checkpoint name for the given model type and epoch\n",
        "\n",
        "    :param checkpoints_base_path: Path to directory to store checkpoints in\n",
        "    :param model: PyTorch model\n",
        "    :return: Checkpoint path and name\n",
        "    \"\"\"\n",
        "    return os.path.join(checkpoints_base_path, f\"chkpt-{epoch}\" + (\"-light\" if new_type else \"\") + \".pt\")\n"
      ],
      "metadata": {
        "id": "iAPTxaXCMz8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Scripts"
      ],
      "metadata": {
        "id": "NR1hYUIZRLpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "# Ensure that folder LLaMaTrain exists in My Drive, with tokenizer.model, tiny_train.jsonl, and tiny_val.jsonl before running the following training code\n",
        "from google.colab import drive\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "gdrive_path = \"/gdrive/MyDrive/LLaMaTrain/\"\n",
        "artifacts_path = gdrive_path + \"artifacts/\""
      ],
      "metadata": {
        "id": "whRBJTgnQaZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_data(\n",
        "    storage_base_path: str,\n",
        "    tokenizer_path: str,\n",
        "    train_path: str,\n",
        "    val_path: str,\n",
        "    initial_chkpt: Optional[str] = None,\n",
        "    num_train: int = 20000,\n",
        "    num_val: int = 10000,\n",
        "    max_seq_len: int = 512,\n",
        "    batch_size: int = 16,\n",
        "    new_chkpt_format: bool = False,\n",
        "    **model_args\n",
        ") -> Tuple[XFormersTransformer, Tokenizer, DataLoader, DataLoader]:\n",
        "    \"\"\"\n",
        "    Load a model and train and val dataloaders for training, evaluation, or generation\n",
        "    \"\"\"\n",
        "    assert os.path.isfile(storage_base_path + tokenizer_path), \"LLaMa tokenizer pretrained model file required\"\n",
        "    assert os.path.isfile(storage_base_path + train_path), \"Train data subset in JSONL format required\"\n",
        "    assert os.path.isfile(storage_base_path + val_path), \"Validation data subset in JSONL format required\"\n",
        "\n",
        "    # Load model\n",
        "    torch.cuda.empty_cache()\n",
        "    model, tokenizer = load_llama(\n",
        "        tokenizer_path=storage_base_path + tokenizer_path,\n",
        "        initial_chkpt=initial_chkpt,\n",
        "        use_xformers=True,\n",
        "        new_chkpt=new_chkpt_format,\n",
        "        max_seq_len=max_seq_len,\n",
        "        **model_args\n",
        "    )\n",
        "\n",
        "    # Load data\n",
        "    train_set, val_set, _ = load_pile_dataset(\n",
        "        tokenizer,\n",
        "        storage_base_path + train_path,\n",
        "        storage_base_path + val_path,\n",
        "        num_train=num_train,\n",
        "        num_val=num_val,\n",
        "        max_seq_len=max_seq_len,\n",
        "    )\n",
        "\n",
        "    train_dataloader, val_dataloader, _ = get_pile_dataloaders(\n",
        "        train_set,\n",
        "        val_set,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    return model, tokenizer, train_dataloader, val_dataloader"
      ],
      "metadata": {
        "id": "Jhu9WwfmP7rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Batches"
      ],
      "metadata": {
        "id": "X5u67M0QENfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL CODE - NOT REQUIRED FOR TRAINING\n",
        "# Show example batches from Pile dataset\n",
        "\n",
        "model, _, train_dataloader, val_dataloader = load_model_and_data(\n",
        "    gdrive_path, \n",
        "    tokenizer_path=\"tokenizer.model\",\n",
        "    train_path=\"data-750k.jsonl\",\n",
        "    val_path=\"tiny_val.jsonl\",\n",
        "    num_train=750000,\n",
        "    num_val=10000,\n",
        "    batch_size=4\n",
        ")\n",
        "\n",
        "print(\"Example train batch\")\n",
        "print(next(iter(train_dataloader)))\n",
        "print(\"Example val batch\")\n",
        "print(next(iter(val_dataloader)))"
      ],
      "metadata": {
        "id": "AbwKBPH8XLXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "NpHolcdGD94T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Optional\n",
        "import torch\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# from src.main.llama import XFormersTransformer, Tokenizer, load_llama\n",
        "# from src.main.util import get_pile_dataloaders, load_pile_dataset\n",
        "# from src.main.util import compute_loss\n",
        "# from src.main.util import save_checkpoint, load_checkpoint, generate_checkpoint_name\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def train(\n",
        "        model: XFormersTransformer,\n",
        "        tokenizer: Tokenizer,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        epochs: int,\n",
        "        lr: float,\n",
        "        weight_decay: float,\n",
        "        grad_clip: float = 1.0,  # clipping for all gradients\n",
        "        chkpt_dir: str = None,  # checkpoint directory\n",
        "        batch_save_freq: int = -1,  # save after this many batches. -1 means only saving at the end of an epoch\n",
        "):\n",
        "    # Main training loop, including checkpointing\n",
        "    if not os.path.isdir(chkpt_base):\n",
        "        os.makedirs(chkpt_base)\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    loss_fn = CrossEntropyLoss(ignore_index=tokenizer.eos_id)  # ignore padding\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        try:\n",
        "            for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
        "                # compute loss between predictions (logits) and tokens\n",
        "                # each prediction corresponds to the next token, so we shift tokens by one\n",
        "                tokens = batch.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                loss = compute_loss(model, tokens, loss_fn)  # compute logits on all using all but last token\n",
        "                tokens.cpu()  # ensure tokens can be garbage collected\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                if batch_save_freq > 0 and (i+1) % batch_save_freq == 0:\n",
        "                    # Save checkpoint\n",
        "                    chkpt_path = generate_checkpoint_name(chkpt_dir, f\"{epoch+1}-batch-{i}\", True)\n",
        "                    torch.save(model.state_dict(), chkpt_path)\n",
        "                    \n",
        "                    val_loss = evaluate(model, val_loader, loss_fn)\n",
        "\n",
        "                    # Print batch summary\n",
        "                    print(f\"Epoch {epoch+1}. Batch {i}. Train loss: {train_loss/i}. Val loss: {val_loss}\")\n",
        "        finally:\n",
        "            # Save checkpoint\n",
        "            chkpt_path = generate_checkpoint_name(chkpt_dir, f\"{epoch+1}-end\", True)\n",
        "            torch.save(model.state_dict(), chkpt_path)\n",
        "            # garbage collect to process next batch\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        val_loss = evaluate(model, val_loader, loss_fn)\n",
        "\n",
        "        # Print summary\n",
        "        train_loss /= len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}. Train loss: {train_loss}. Val loss: {val_loss}\")\n",
        "\n",
        "\n",
        "# Checkpoints\n",
        "chkpt_base = gdrive_path + \"checkpoints/\" + \"dim-256-heads-8-layers-4-huge-run-part-3/\"\n",
        "ichkpt_path = gdrive_path + \"checkpoints/\" + \"dim-256-heads-8-layers-4-huge-run-part-2/\" + \"chkpt-1-batch-17999-light.pt\" # initial checkpoint, if any\n",
        "\n",
        "# Training parameters\n",
        "epochs = 2\n",
        "lr = 3e-4\n",
        "weight_decay = 0.01\n",
        "\n",
        "model, tokenizer, train_dataloader, val_dataloader = load_model_and_data(\n",
        "    gdrive_path, \n",
        "    tokenizer_path=\"tokenizer.model\",\n",
        "    train_path=\"data-750k.jsonl\",\n",
        "    val_path=\"tiny_val.jsonl\",\n",
        "    num_train=1500000,\n",
        "    num_val=10000,\n",
        "    batch_size=32,\n",
        "    dim=256,\n",
        "    n_layers=8,\n",
        "    n_heads=4,\n",
        "    initial_chkpt=ichkpt_path,\n",
        "    new_chkpt_format=True\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Train model\n",
        "    train(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        lr=lr,\n",
        "        epochs=epochs,\n",
        "        weight_decay=weight_decay,\n",
        "        chkpt_dir=chkpt_base,\n",
        "        batch_save_freq=3000\n",
        "    )\n",
        "finally:\n",
        "    # Ensure model is on CPU so it can be garbage collected\n",
        "    print(\"Cleaning up...\")\n",
        "    model.cpu()\n",
        "    del model, tokenizer, train_dataloader, val_dataloader\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "xnxb1CRxRMby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Cz_X6Hh4Oss6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and dataloaders\n",
        "\n",
        "chkpt_base = gdrive_path + \"checkpoints/\" + \"dim-256-heads-8-layers-8-big-run/\"\n",
        "chkpt_name = \"chkpt-1-end-light.pt\"\n",
        "\n",
        "assert os.path.isfile(chkpt_base + chkpt_name), \"Initial checkpoint required to eval\"\n",
        "\n",
        "model, tokenizer, _, val_dataloader = load_model_and_data(\n",
        "    gdrive_path, \n",
        "    tokenizer_path=\"tokenizer.model\",\n",
        "    train_path=\"tiny_train.jsonl\",\n",
        "    val_path=\"tiny_val.jsonl\",\n",
        "    num_train=0,\n",
        "    num_val=10000,\n",
        "    batch_size=64,\n",
        "    dim=256,\n",
        "    n_layers=8,\n",
        "    n_heads=8,\n",
        "    initial_chkpt=chkpt_base + chkpt_name,\n",
        "    new_chkpt_format=True\n",
        ")\n",
        "\n",
        "\n",
        "# Perform evaluation\n",
        "model.to(device)\n",
        "model.eval()\n",
        "loss_fn = CrossEntropyLoss(ignore_index=tokenizer.eos_id)\n",
        "print(f\"Validation loss on model: {evaluate(model, val_dataloader, loss_fn)}\")\n",
        "model.cpu()\n",
        "torch.cuda.empty_cache()\n",
        "del model"
      ],
      "metadata": {
        "id": "UrCPhIHhOuKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "3wRb2g0_D_1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLaMAInference:\n",
        "    def __init__(\n",
        "        self, \n",
        "        model: XFormersTransformer, \n",
        "        tokenizer: Tokenizer\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        max_gen_len: int = 512,\n",
        "        temperature: float = 0.8,\n",
        "        top_p: float = 0.95,\n",
        "    ) -> List[str]:\n",
        "        self.model.to(device)\n",
        "        batch_size = len(prompts)\n",
        "\n",
        "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
        "        min_prompt_size = min([len(t) for t in prompt_tokens])\n",
        "        max_prompt_size = max([len(t) for t in prompt_tokens])\n",
        "        assert max_prompt_size < max_gen_len\n",
        "\n",
        "        tokens = torch.full((batch_size, max_gen_len), self.tokenizer.eos_id).cuda().long()\n",
        "        for k, t in enumerate(prompt_tokens):\n",
        "            tokens[k, :len(t)] = torch.tensor(t).long()\n",
        "        input_text_mask = (tokens != self.tokenizer.eos_id)\n",
        "\n",
        "        start_pos = min_prompt_size\n",
        "        self.model.eval()\n",
        "        for cur_pos in range(start_pos, 100):\n",
        "            logits = self.model(tokens)[:, cur_pos, :]\n",
        "            if temperature > 0:\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                next_token = sample_top_p(probs, top_p)\n",
        "            else:\n",
        "                next_token = torch.argmax(logits, dim=-1)\n",
        "            next_token = next_token.reshape(-1)\n",
        "\n",
        "            next_token = torch.where(\n",
        "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
        "            )\n",
        "            tokens[:, cur_pos] = next_token\n",
        "\n",
        "        decoded = []\n",
        "        for i, t in enumerate(tokens.tolist()):\n",
        "            decoded.append(self.tokenizer.decode(t))\n",
        "        return decoded\n",
        "\n",
        "\n",
        "def sample_top_p(probs, p):\n",
        "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "    mask = probs_sum - probs_sort > p\n",
        "    probs_sort[mask] = 0.0\n",
        "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "    next_token = torch.gather(probs_idx, -1, next_token)\n",
        "    return next_token"
      ],
      "metadata": {
        "id": "6adysCBYEBcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and dataloaders\n",
        "\n",
        "chkpt_base = gdrive_path + \"checkpoints/\" + \"dim-256-heads-8-layers-8-big-run/\"\n",
        "chkpt_name = \"chkpt-1-end-light.pt\"\n",
        "\n",
        "assert os.path.isfile(chkpt_base + chkpt_name), \"Initial checkpoint required to eval\"\n",
        "\n",
        "model, tokenizer, _, _ = load_model_and_data(\n",
        "    gdrive_path, \n",
        "    tokenizer_path=\"tokenizer.model\",\n",
        "    train_path=\"tiny_train.jsonl\",\n",
        "    val_path=\"tiny_val.jsonl\",\n",
        "    num_train=0,\n",
        "    num_val=0,\n",
        "    batch_size=64,\n",
        "    dim=256,\n",
        "    n_layers=8,\n",
        "    n_heads=8,\n",
        "    initial_chkpt=chkpt_base + chkpt_name,\n",
        "    new_chkpt_format=True\n",
        ")\n",
        "\n",
        "inference_model = LLaMAInference(model, tokenizer)\n",
        "\n",
        "prompts = [\"The history of Spain can be summarized through the reigns of various kings and queens which\", \"Our Golf Umbrellas will score a Hole-in-One with your loyal golf-loving customers and clients. Every time they hit the greens with one of your promotional golf umbrellas\", \"The meaning of life\", \"The cat sat\"]\n",
        "\n",
        "try:\n",
        "  responses = inference_model.generate(prompts, temperature=0.8)\n",
        "finally:\n",
        "  # Cleanup before exiting\n",
        "  inference_model.model.cpu()\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "for response in responses:\n",
        "  print(response)"
      ],
      "metadata": {
        "id": "sWxhtbu6DPIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}